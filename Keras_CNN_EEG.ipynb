import pandas as pd
from keras.models import Sequential
from keras import layers,metrics, regularizers
from sklearn import metrics
from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense
from sklearn.model_selection import train_test_split
import numpy as np
import seaborn as sns
from scipy import stats
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical

data =pd.read_csv('C:/Users/skalathu/Downloads/Train/A1/raw/test7_bp.csv')
data_test=pd.read_csv('C:/Users/skalathu/Downloads/Test/A1/raw/test7_bp.csv')

band = [4,8,12,16,25,45]
window_size = 256 #Averaging band power of 2 sec
step_size = 16 #Each 0.125 sec update once
sample_rate = 128 #Sampling rate of 128 Hz
#def feature_normalize(dataset):
 #   return (dataset - np.mean(dataset, axis=0))/np.std(dataset, axis=0)

data2 = data[data.columns[0:72]] 
data2_test=data_test[data.columns[0:72]] 
trainX = data2[data2.columns[0:71]] 
x1_test=data2_test[data2_test.columns[0:71]]
#print(trainX)
scaler = StandardScaler()
trainX = scaler.fit_transform(trainX)
x1_test = scaler.fit_transform(x1_test)
trainy = data2[["Y"]]
y_train=to_categorical(trainy)
y1_test=data2_test[["Y"]]
y1_test=to_categorical(y1_test)
#x_train, x_test, y_train, y_test = train_test_split(trainX, trainy, test_size = 0.2,random_state = 120)
x_train = np.array(trainX)
#x_test=np.array(x_test)
x1_test=np.array(x1_test)
#y_train = to_categorical(y_train)
#y_test=to_categorical(y_test)
#y1_test=to_categorical(y1_test)
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)
#x_test= x_test.reshape(x_test.shape[0], x_test.shape[1],1)
x1_test= x1_test.reshape(x1_test.shape[0], x1_test.shape[1],1)
print(y1_test.shape)
def show_confusion_matrix(validations, predictions):

    matrix = metrics.confusion_matrix(validations, predictions)
    plt.figure(figsize=(6, 4))
    sns.heatmap(matrix,
                cmap="coolwarm",
                linecolor='white',
                linewidths=1,
                xticklabels=LABELS,
                yticklabels=LABELS,
                annot=True,
                fmt="d")
    plt.title("Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.show()

batch_size = 128
num_classes = 2
epochs = 75
input_shape=(x_train.shape[1], 1)
LABELS = [0,1]

    
#verbose, epochs, batch_size = 0, 10, 32
#n_timesteps = x_train.shape[1]
#n_features=x_train.shape[2]
#n_outputs=2
model = Sequential()
model.add(Convolution1D(128, 2, activation="tanh", input_shape=input_shape))
                       # kernel_regularizer=regularizers.l2(l2=0.01)))
model.add(BatchNormalization())
model.add(Dense(16, activation="tanh"))
model.add(MaxPooling1D())
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation = 'tanh'))
model.compile(loss = 'binary_crossentropy', 
     optimizer = "adam",               
              metrics = ['accuracy'])
#model.add(Dense(num_classes, activation = 'softmax'))
#model.compile(loss = 'binary_crossentropy', optimizer = "adam", metrics =['accuracy'])
model.summary()
model.fit(x_train, y_train, batch_size=batch_size,epochs=epochs, verbose=1)

#acc_train = model.evaluate(x_test, y_test)
#acc_test = model.evaluate(x1_test, y1_test)
pred_val = model.predict(x1_test)
#print("Train Loss:", acc_train[0], " Train Accuracy:", acc_train[1])
#print("Test Loss:", acc_test[0], " Test Accuracy:", acc_test[1])
pred = np.argmax(pred_val,axis=1)
actual = np.argmax(y1_test,axis=1)
print(pred)
print(actual)
show_confusion_matrix(pred, actual)
TN, FP, FN, TP = metrics.confusion_matrix(actual, pred).ravel()
print("True Negatives: ",TN)
print("False Positives: ",FP)
print("False Negatives: ",FN)
print("True Positives: ",TP)
FAR=FP/(FP+TN)
FRR=FN/(TP+FN)
print(FAR,FRR)
print(classification_report(actual, pred))
